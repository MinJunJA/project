{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전체 음성 데이터 전처리\n",
    "\n",
    "전체 아기울음 소리 음성 데이터(.wav)에 대해 전처리를 수행한다.\n",
    "\n",
    "음성의 진폭과 주파수에 따른 특징이 잘 드러나도록 세그먼트 분할, 노이즈 제거, 등의 작업을 1~7 번 과정에서 수행하며\n",
    "\n",
    "8~10 번 과정에서 음성 길이 통일, masking, 등의 과정을 통해 동일한 크기의 벡터 생성 및 데이터 증강을 수행한다.\n",
    "\n",
    "본 문서는 수집된 전체 음성 데이터에 대해 수행되며 각 단계에 대한 필요성 언급 및 시각화를 하지 않는다. 이는 전체 데이터를 효과적으로 전처리하기 위함이며 각 단계의 필요성 및 시각적 자료는 trans_data_preview.ipynb 파일에 명시되어 있다.\n",
    "\n",
    "<br>\n",
    "\n",
    "전처리 순서는 아래와 같다.\n",
    "\n",
    "1. 사용할 수 없는 데이터는 제외한 다음 상위 7개의 state를 가지는 음성만을 가져온다.\n",
    "\n",
    "2. sample rate를 16000으로 통일한다.\n",
    "\n",
    "3. 로그 멜 스펙트럼을 통해 음성의 파워를 측정한다.\n",
    "\n",
    "4. 파워의 지역 최솟값을 기준으로 음성을 분할한다.\n",
    "\n",
    "5. 분할된 음성의 앞뒤 화이트 노이즈(특정 임계값 이하의 에너지를 가지는 시점)을 제거한다.\n",
    "\n",
    "6. 분할된 음성이 노이즈를 제거하여 패턴을 보다 잘 드러나게 한다.\n",
    "\n",
    "7. 음성 정규화를 수행한다.\n",
    "\n",
    "8. Padding과 Trimming을 통해 평균 세그멘테이션 길이를 가지도록 음성의 길이를 통일한다.\n",
    "\n",
    "9. 음성의 유사도를 측정하여 하위 10%의 데이터는 제외한다.\n",
    "\n",
    "10. 유사도 상위 10% 데이터에 대하여 SpecAugment를 적용하여 Training 데이터에 대해 마스킹 작업을 수행한다. 각 파일 당 마스킹 파일 2개를 생성한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load packages\n",
    "import os\n",
    "import sys\n",
    "import wave\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Path\n",
    "main_path = os.path.join(os.getcwd().rsplit(\n",
    "    'baby-cry-classification')[0], 'baby-cry-classification')\n",
    "data_path = os.path.join(main_path, 'data')\n",
    "temp_data_path = os.path.join(main_path, 'temp_data')\n",
    "csv_path = os.path.join(main_path, 'origin_data_info.csv')\n",
    "origin_data_path = os.path.join(main_path, 'origin_data')\n",
    "\n",
    "sys.path.append(main_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터\n",
    "\n",
    "search_in_sec = 3   # 파워값을 측정하는 시간 간격\n",
    "\n",
    "state_list = ['sad', 'hug', 'diaper', 'hungry',\n",
    "              'sleepy', 'awake', 'uncomfortable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 함수들을 정의한다.\n",
    "def show_spectrum(y, sr):\n",
    "    # STFT 계산\n",
    "    D = librosa.amplitude_to_db(abs(librosa.stft(y)), ref=np.max)\n",
    "\n",
    "    # 시간과 주파수 축을 위한 값들 계산\n",
    "    times = np.linspace(0, len(y)/sr, num=D.shape[1], endpoint=False)\n",
    "    freqs = librosa.fft_frequencies(sr=sr, n_fft=2048)\n",
    "\n",
    "    # 스펙트럼 시각화\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.imshow(D, aspect='auto', origin='lower', extent=[\n",
    "               times.min(), times.max(), freqs.min(), freqs.max()])\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.ylabel('Frequency (Hz)')\n",
    "    plt.title('Spectrogram')\n",
    "    plt.yscale('log')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_mel_power(power, dot_list=[], dot_color='red', dot_label=''):\n",
    "    # 파워 시각화\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.plot(power)\n",
    "    plt.title(\"Log Mel Spectrum Power\")\n",
    "    plt.xlabel(\"Time Frame\")\n",
    "    plt.ylabel(\"Power\")\n",
    "\n",
    "    if (len(dot_list) > 0):\n",
    "        plt.scatter(dot_list, power[dot_list],\n",
    "                    color=dot_color, label=dot_label)\n",
    "        plt.legend()\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_masked_mel(db_masked_mel):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(db_masked_mel, origin='lower', aspect='auto',\n",
    "               extent=[0, db_masked_mel.shape[1], 0, db_masked_mel.shape[0]])\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Realistically Masked Mel spectrogram (using matplotlib)')\n",
    "    plt.xlabel('Time frames')\n",
    "    plt.ylabel('Mel frequency bins')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1,2 번 과정을 수행한다.\n",
    "from trans_data import create_state_folder, get_state_file_list\n",
    "\n",
    "create_state_folder(origin_data_path, csv_path, temp_data_path, etc=False)\n",
    "file_list = get_state_file_list(temp_data_path, state_list, '.wav')\n",
    "\n",
    "file_list = get_state_file_list(temp_data_path, state_list, '.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3번 과정의 함수를 정의한다.\n",
    "from scipy.signal import stft\n",
    "\n",
    "\n",
    "def read_wav(file_path):\n",
    "    with wave.open(file_path, 'r') as wav_file:\n",
    "        n_channels, sampwidth, framerate, n_frames, comptype, compname = wav_file.getparams()\n",
    "        audio_data = wav_file.readframes(n_frames)\n",
    "        audio_data = np.frombuffer(audio_data, dtype=np.int16)\n",
    "\n",
    "    # Stereo 파일의 경우 mono로 변환\n",
    "    if n_channels == 2:\n",
    "        audio_data = (audio_data[::2] + audio_data[1::2]) / 2\n",
    "\n",
    "    return [audio_data, {'n_channels': n_channels,\n",
    "                         'sampwidth': sampwidth,\n",
    "                         'framerate': framerate,\n",
    "                         'n_frames': n_frames,\n",
    "                         'comptype': comptype,\n",
    "                         'compname': compname}]\n",
    "\n",
    "\n",
    "def mel_filter_bank(num_filters, fft_size, sample_rate):\n",
    "    \"\"\"\n",
    "    멜 필터뱅크 생성\n",
    "    \"\"\"\n",
    "    # 멜 스케일과 헤르츠 스케일 간의 변환 함수\n",
    "    def hz_to_mel(hz): return 2595 * np.log10(1 + hz / 700)\n",
    "    def mel_to_hz(mel): return 700 * (10**(mel / 2595) - 1)\n",
    "\n",
    "    # 멜 스케일로 끝점 설정\n",
    "    mel_end = hz_to_mel(sample_rate / 2)\n",
    "    mel_points = np.linspace(hz_to_mel(0), mel_end, num_filters + 2)\n",
    "    hz_points = mel_to_hz(mel_points)\n",
    "\n",
    "    # FFT 주파수 인덱스로 변환\n",
    "    bin_points = np.floor((fft_size + 1) * hz_points / sample_rate).astype(int)\n",
    "\n",
    "    # 필터뱅크 생성\n",
    "    filters = np.zeros((num_filters, fft_size // 2 + 1))\n",
    "    for i in range(1, num_filters + 1):\n",
    "        filters[i - 1, bin_points[i - 1]:bin_points[i]] = \\\n",
    "            (np.arange(bin_points[i - 1], bin_points[i]) -\n",
    "             bin_points[i - 1]) / (bin_points[i] - bin_points[i - 1])\n",
    "        filters[i - 1, bin_points[i]:bin_points[i + 1]] = 1 - \\\n",
    "            (np.arange(bin_points[i], bin_points[i + 1]) -\n",
    "             bin_points[i]) / (bin_points[i + 1] - bin_points[i])\n",
    "    return filters\n",
    "\n",
    "\n",
    "def get_log_mel_spectogram(audio_data, framerate, num_filters=40, nperseg=2048, noverlap=1024, nfft=2048):\n",
    "    # STFT 계산\n",
    "    _, _, Zxx = stft(audio_data, fs=framerate, nperseg=nperseg,\n",
    "                     noverlap=noverlap, nfft=nfft)\n",
    "    magnitude = np.abs(Zxx)\n",
    "\n",
    "    # 로그 멜 스펙트럼 추출\n",
    "    num_filters = num_filters\n",
    "    filters = mel_filter_bank(num_filters, 2048, framerate)\n",
    "    mel_spectrum = np.dot(filters, magnitude)\n",
    "    log_mel_spectrum = np.log(mel_spectrum + 1e-9)  # log 0을 피하기 위한 작은 값 추가\n",
    "    return log_mel_spectrum\n",
    "\n",
    "\n",
    "def get_mel_power(log_mel_spectrum):\n",
    "    # 로그 멜 스펙트럼의 파워 계산\n",
    "    return np.sum(log_mel_spectrum**2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4번 과정의 함수를 정의한다.\n",
    "def get_interval_min_sec(power, sec: int = 1):\n",
    "    interval_sec = int(10 * sec)\n",
    "    min_sec_list = []\n",
    "    for i in range(0, len(power) + 1, interval_sec):\n",
    "        argsort = np.argsort(power[i:i+interval_sec])\n",
    "        if len(argsort) < 1:\n",
    "            break\n",
    "        min_sec = i + argsort[0]\n",
    "        min_sec_list.append(min_sec)\n",
    "    return min_sec_list\n",
    "\n",
    "\n",
    "def split_audio_on_indices(audio_data, power, indices):\n",
    "    \"\"\"\n",
    "    주어진 인덱스를 기준으로 오디오 데이터를 여러 부분으로 나누는 함수.\n",
    "\n",
    "    Parameters:\n",
    "    - audio_data: 오디오 데이터 배열\n",
    "    - indices: 분할할 시점의 리스트\n",
    "\n",
    "    Returns:\n",
    "    - audio_segments: 나눈 오디오 데이터의 리스트\n",
    "    \"\"\"\n",
    "    # STFT를 통해 구한 시점을 오디오 샘플의 시점으로 변환\n",
    "    nperseg = 2048\n",
    "    noverlap = 1024\n",
    "    samples_per_frame = (len(audio_data) - nperseg) / (len(power) - 1)\n",
    "    sample_indices = [int(index * samples_per_frame) for index in indices]\n",
    "\n",
    "    audio_segments = []\n",
    "    prev_index = 0\n",
    "    for index in sample_indices:\n",
    "        segment = audio_data[prev_index:index]\n",
    "        if len(segment) != 0:\n",
    "            audio_segments.append(segment)\n",
    "        prev_index = index\n",
    "    audio_segments.append(audio_data[prev_index:])  # 마지막 세그먼트 추가\n",
    "\n",
    "    return audio_segments\n",
    "\n",
    "\n",
    "def save_audio_segments_as_wav(audio_segments, output_dir, prefix, sampwidth, framerate):\n",
    "    \"\"\"\n",
    "    주어진 오디오 세그먼트들을 WAV 파일로 저장하는 함수.\n",
    "\n",
    "    Parameters:\n",
    "    - audio_segments: 나눈 오디오 데이터의 리스트\n",
    "    - output_dir: 출력 디렉토리 경로\n",
    "    - prefix: 저장할 파일의 접두사\n",
    "\n",
    "    Returns:\n",
    "    - filepaths: 저장된 파일의 경로 리스트\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    filepaths = []\n",
    "    for i, segment in enumerate(audio_segments):\n",
    "        filename = f\"{prefix}_{uuid4()}.wav\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "        with wave.open(filepath, 'wb') as wav_file:\n",
    "            wav_file.setnchannels(1)\n",
    "            wav_file.setsampwidth(sampwidth)\n",
    "            wav_file.setframerate(framerate)\n",
    "            wav_file.writeframes(segment.tobytes())\n",
    "        filepaths.append(filepath)\n",
    "\n",
    "    return filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5,6,7번 과정의 함수를 정의하거나 불러온다.\n",
    "\n",
    "from trans_data import trim_audio, reduced_base_noise\n",
    "from scipy.io import wavfile\n",
    "\n",
    "\n",
    "def normalize_and_save_audio(input_path, output_path=None, inplace=True):\n",
    "    \"\"\"\n",
    "    Load an audio file from the given input path, normalize it, and save the normalized audio to the given output path.\n",
    "\n",
    "    Parameters:\n",
    "    - input_path: Path to the input audio file.\n",
    "    - output_path: Path to save the normalized audio file.\n",
    "    - inplace: 이미 존재하는 파일에 덮어쓴다.\n",
    "    \"\"\"\n",
    "    if inplace == False and output_path == None:\n",
    "        raise ValueError(f'output path must be defined if inplace is False.')\n",
    "\n",
    "    # Load the audio file\n",
    "    y, sr = librosa.load(input_path, sr=None)\n",
    "\n",
    "    # Normalize the audio data\n",
    "    y_normalized = y / np.max(np.abs(y))\n",
    "\n",
    "    # Convert float to int16 (since wavfile.write requires int values)\n",
    "    y_normalized_int16 = (y_normalized * 32767).astype(np.int16)\n",
    "\n",
    "    if inplace:\n",
    "        os.remove(input_path)\n",
    "        output_path = input_path\n",
    "\n",
    "    # Save the normalized audio data to the specified output path\n",
    "    wavfile.write(output_path, sr, y_normalized_int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3238 [00:00<?, ?it/s]/Users/jaewone/ENTER/envs/tf25/lib/python3.9/site-packages/numba/cpython/hashing.py:482: UserWarning: FNV hashing is not implemented in Numba. See PEP 456 https://www.python.org/dev/peps/pep-0456/ for rationale over not using FNV. Numba will continue to work, but hashes for built in types will be computed using siphash24. This will permit e.g. dictionaries to continue to behave as expected, however anything relying on the value of the hash opposed to hash as a derived property is likely to not work as expected.\n",
      "  warnings.warn(msg)\n",
      "  2%|▏         | 70/3238 [00:08<05:26,  9.71it/s]/Users/jaewone/ENTER/envs/tf25/lib/python3.9/site-packages/noisereduce/noisereduce.py:306: RuntimeWarning: invalid value encountered in true_divide\n",
      "  sig_mult_above_thresh = (abs_sig_stft - sig_stft_smooth) / sig_stft_smooth\n",
      "/var/folders/10/9_p98m6j42n84y4wmf_k0krc0000gn/T/ipykernel_97901/1225871274.py:22: RuntimeWarning: invalid value encountered in true_divide\n",
      "  y_normalized = y / np.max(np.abs(y))\n",
      "100%|██████████| 3238/3238 [11:15<00:00,  4.79it/s]\n"
     ]
    }
   ],
   "source": [
    "# 3 ~ 7 번 과정을 수행한다.\n",
    "\n",
    "for i in tqdm(range(len(file_list))):\n",
    "    file_state = file_list[i].rsplit('/', 2)[1]\n",
    "\n",
    "    # 3. 로그 멜 스펙트럼을 통해 음성의 파워를 측정한다.\n",
    "\n",
    "    # 파일을 읽어온다.\n",
    "    audio_data, audio_status = read_wav(file_list[i])\n",
    "\n",
    "    # 로그 멜 스펙트럼을 추출한 뒤\n",
    "    log_mel_spectogram = get_log_mel_spectogram(\n",
    "        audio_data, audio_status['framerate'])\n",
    "\n",
    "    # 파워를 측정한다.\n",
    "    power = get_mel_power(log_mel_spectogram)\n",
    "\n",
    "    # 4. 파워의 지역 최솟값을 기준으로 음성을 분할한다.\n",
    "\n",
    "    # 파워의 지역 최솟값의 시점들을 구한다.\n",
    "    min_sec_list = get_interval_min_sec(power, sec=search_in_sec)\n",
    "\n",
    "    # 지역 최솟값을 기준으로 오디오(numpy)값을 분할한다.\n",
    "    audio_segments_sec = split_audio_on_indices(\n",
    "        audio_data, power, min_sec_list)\n",
    "\n",
    "    # 분할된 값을 data_path에 저장한다.\n",
    "    saved_filepaths = save_audio_segments_as_wav(\n",
    "        audio_segments=audio_segments_sec,\n",
    "        output_dir=data_path,\n",
    "        prefix=file_state,\n",
    "        sampwidth=audio_status['sampwidth'],\n",
    "        framerate=audio_status['framerate'],\n",
    "    )\n",
    "\n",
    "    # # 분할된 음성 중 search_in_sec의 절반보다 긴 음성들에 대해 5,6 과정을 진행한다.\n",
    "    total_error_files = []\n",
    "    for file_path in saved_filepaths:\n",
    "        # WAV 파일 로드\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "        # 파일의 길이(초) 출력\n",
    "        origin_duration = librosa.get_duration(y=y, sr=sr)\n",
    "        if origin_duration > 1:\n",
    "            # 5. 분할된 음성의 앞뒤 화이트 노이즈(특정 임계값 이하의 에너지를 가지는 시점)을 제거한다.\n",
    "            error_files = trim_audio(file_path, inplace=True, frame_size=1000)\n",
    "            if (len(error_files) > 0):\n",
    "                total_error_files += error_files\n",
    "\n",
    "            # 6. 분할된 음성이 노이즈를 제거하여 패턴을 보다 잘 드러나게 한다.\n",
    "            reduced_base_noise(file_path, inplace=True)\n",
    "\n",
    "            # 7. 음성 정규화를 수행한다.\n",
    "            normalize_and_save_audio(file_path, inplace=True)\n",
    "        else:\n",
    "            os.remove(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 음성 길이 규정\n",
    "\n",
    "위 과정을 통해 음성의 특징을 가지는 각각의 전처리된 세그먼트가 wav 파일 형태로 저장되었다.\n",
    "\n",
    "모델의 입력 벡터로 구성하기 위해서는 모든 음성의 길이가 동일해야 함으로 모든 세그먼트의 평균 길이를 구한 뒤 Padding과 Trimming 과정을 통해 평균 길이로 음성 길이를 통일하고자 한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평균 세그먼트 길이 측정\n",
    "\n",
    "아래 과정을 통해 평균 세그먼트의 (음성)길이는 1.86 임을 알았다. 이에 음성을 2초로 통일하고자 한다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average: 1.8620317905047654\n",
      "사용할 음성의 길이: 2\n"
     ]
    }
   ],
   "source": [
    "file_list = [os.path.join(data_path, file) for file in os.listdir(data_path)]\n",
    "duration_list = []\n",
    "for file_path in file_list:\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "    duration = librosa.get_duration(y=y, sr=sr)\n",
    "    if duration < 1:\n",
    "        os.remove(file_path)\n",
    "    else:\n",
    "        duration_list.append(duration)\n",
    "sum = np.array(duration_list).sum()\n",
    "avg = sum / len(duration_list)\n",
    "avg_duration = round(avg)\n",
    "print(f'Average: {avg}')\n",
    "print(f'사용할 음성의 길이: {avg_duration}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state(hungry, sad, ...etc) 이름을 가지는 폴더로 해당하는 음성 파일을 이동시킨 뒤 음성 파일의 이름을 state_숫자 형태로 변환하여\n",
    "# 사용자가 파일명을 통한 state 확인을 돕는다.\n",
    "from utils.os import move_file\n",
    "\n",
    "\n",
    "def move_files_by_state(data_path, state_list):\n",
    "    file_list = [os.path.join(data_path, file)\n",
    "                 for file in os.listdir(data_path)]\n",
    "    new_file_list = []\n",
    "\n",
    "    for state in state_list:\n",
    "        os.mkdir(os.path.join(data_path, state))\n",
    "\n",
    "    for file in file_list:\n",
    "        file_name = file.rsplit('/', 1)[1]\n",
    "        file_state = file_name.split('_')[0]\n",
    "        # new_file_path = f'{data_path}/{file_state}/{file_name}'\n",
    "        new_file_path = os.path.join(data_path, file_state, file_name)\n",
    "        move_file(file, new_file_path)\n",
    "        new_file_list.append(new_file_path)\n",
    "    return new_file_list\n",
    "\n",
    "\n",
    "new_file_list = move_files_by_state(data_path, state_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8번 과정을 수행한다.\n",
    "\n",
    "def pad_audio_to_length(input_path, target_duration, output_path=None, inplace=False):\n",
    "    \"\"\"\n",
    "    wav 파일을 target_duration 길이로 맞춘다. \n",
    "    음성의 중심을 기준으로 target_duration보다 짧을 경우 앞뒤에 Padding을 추가하며 \n",
    "    음성의 중심을 기준으로 target_duration보다 길 경우 앞뒤를 자른다.\n",
    "    \"\"\"\n",
    "\n",
    "    if inplace == False and output_path == None:\n",
    "        raise ValueError(f'output path must be defined if inplace is False.')\n",
    "\n",
    "    # WAV 파일 읽기\n",
    "    y, sr = librosa.load(input_path, sr=None)\n",
    "\n",
    "    # 현재 오디오 파일의 길이 확인\n",
    "    current_duration = len(y) / sr\n",
    "\n",
    "    # target_duration과 비교하여 패딩 또는 trimming 필요 여부 확인\n",
    "    total_samples = int(target_duration * sr)\n",
    "\n",
    "    if len(y) < total_samples:\n",
    "        # 패딩 필요\n",
    "        padding_samples = total_samples - len(y)\n",
    "        left_padding = padding_samples // 2\n",
    "        right_padding = padding_samples - left_padding\n",
    "        processed_audio = np.pad(\n",
    "            y, (left_padding, right_padding), mode='constant')\n",
    "    else:\n",
    "        # trimming 필요\n",
    "        excess_samples = len(y) - total_samples\n",
    "        left_trim = excess_samples // 2\n",
    "        right_trim = excess_samples - left_trim\n",
    "        processed_audio = y[left_trim:-right_trim]\n",
    "\n",
    "    if inplace:\n",
    "        os.remove(input_path)\n",
    "        output_path = input_path\n",
    "\n",
    "    # 변환된 데이터를 WAV 파일로 저장\n",
    "    sf.write(output_path, processed_audio, sr)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "\n",
    "for file_path in new_file_list:\n",
    "    pad_audio_to_length(file_path, avg_duration, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State sad with file counts: 3389\n",
      "State hug with file counts: 1972\n",
      "State diaper with file counts: 1349\n",
      "State hungry with file counts: 2751\n",
      "State sleepy with file counts: 1645\n",
      "State awake with file counts: 1593\n",
      "State uncomfortable with file counts: 1466\n",
      "\n",
      "가장 적게 존재하는 State는 uncomfortable 이며 개수는 1349 이다.\n"
     ]
    }
   ],
   "source": [
    "# 9번 과정을 수행한다 : 음성의 유사도를 측정하여 하위 10%의 데이터는 제외한다.\n",
    "\n",
    "min_state = ''\n",
    "min_count = 100000    # 초기값: 불가능한 아주 큰 수\n",
    "for state in os.listdir(data_path):\n",
    "    count = len(os.listdir(os.path.join(data_path, state)))\n",
    "    print(f'State {state} with file counts: {count}')\n",
    "    if count < min_count:\n",
    "        min_count = count\n",
    "        min_state = state\n",
    "\n",
    "print(f'\\n가장 적게 존재하는 State는 {state} 이며 개수는 {min_count} 이다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9번 과정을 수행한다 : 유사도를 측정한 뒤 전체 파일의 개수를 맞춘다.\n",
    "from trans_data import get_similarities\n",
    "from utils.os import remove_file\n",
    "\n",
    "\n",
    "def delete_dissimilar_wavs(data_path: str, n_limit: int, save_dir=None):\n",
    "    state_list = ['sad', 'hug', 'diaper', 'hungry',\n",
    "                  'sleepy', 'awake', 'uncomfortable']\n",
    "    for state in state_list:\n",
    "        state_path = os.path.join(data_path, state)\n",
    "        state_file_list = [os.path.join(state_path, file)\n",
    "                           for file in os.listdir(state_path)]\n",
    "\n",
    "        # get similarities\n",
    "        sim_file_list = get_similarities(state_file_list)\n",
    "        if (save_dir != None):\n",
    "            np.savetxt(f'{os.path.join(save_dir, state)}_similarity.txt',\n",
    "                       sim_file_list, delimiter=',', fmt='%s')\n",
    "\n",
    "        # 상위 90%를 제외하고 모두 제거한다.\n",
    "        del_file_list = sim_file_list[n_limit:]\n",
    "        for file in del_file_list:\n",
    "            remove_file(file)\n",
    "\n",
    "\n",
    "save_dir = os.path.join(main_path, 'save_similarity')\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "delete_dissimilar_wavs(data_path, min_count, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process uncomfortable_similarity.txt Done.\n",
      "Process sad_similarity.txt Done.\n",
      "Process hug_similarity.txt Done.\n",
      "Process hungry_similarity.txt Done.\n",
      "Process awake_similarity.txt Done.\n",
      "Process diaper_similarity.txt Done.\n",
      "Process sleepy_similarity.txt Done.\n"
     ]
    }
   ],
   "source": [
    "# 9번 과정을 수행한다 : 음성의 유사도를 측정하여 하위 10%의 데이터는 제외한다.\n",
    "# 그리고 유사도 순서대로 파일에 순번을 부과한다.\n",
    "left_file_len = int(min_count * 0.9) + 1\n",
    "\n",
    "save_dir = os.path.join(main_path, 'save_similarity')\n",
    "for txt_file in os.listdir(save_dir):\n",
    "\n",
    "    with open(os.path.join(save_dir, txt_file), 'r') as f:\n",
    "        file_list = f.read().splitlines()\n",
    "\n",
    "    state = txt_file.split('_')[0]\n",
    "\n",
    "    del_file_list = file_list[left_file_len:]\n",
    "    for file in del_file_list:\n",
    "        if os.path.exists(file):\n",
    "            os.remove(file)\n",
    "\n",
    "    use_file_list = file_list[:left_file_len]\n",
    "\n",
    "    # rename\n",
    "    renamed_file_list = []\n",
    "    for i in range(len(use_file_list)):\n",
    "        path = use_file_list[i].rsplit('/', 1)[0]\n",
    "        ex = use_file_list[i].rsplit('.', 1)[1]\n",
    "        renamed_file = f'{path}/{state}_{i+1}.{ex}'\n",
    "        renamed_file_list.append(renamed_file)\n",
    "        os.rename(use_file_list[i], renamed_file)\n",
    "\n",
    "    print(f'Process {txt_file} Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처리가 완료된 유사도 파일들은 삭제한다.\n",
    "from utils.os import remove_path_with_files\n",
    "\n",
    "remove_path_with_files(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [00:30<00:00,  3.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State sad masking done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [00:42<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State hug masking done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [00:37<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State diaper masking done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [00:37<00:00,  3.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State hungry masking done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [00:43<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State sleepy masking done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [00:40<00:00,  2.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State awake masking done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 121/121 [00:47<00:00,  2.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State uncomfortable masking done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 10번 과정을 수행한다.\n",
    "# 유사도 상위 10% 데이터에 대하여 SpecAugment를 적용하여 Training 데이터에 대해 마스킹 작업을 수행한다.\n",
    "\n",
    "def mask_melspectrogram(file_path,\n",
    "                        freq_mask_num=2,\n",
    "                        time_mask_num=2,\n",
    "                        freq_masking_max_percentage=0.15,\n",
    "                        time_masking_max_percentage=0.3):\n",
    "\n",
    "    # 1. Load the audio file\n",
    "    y, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "    # 2. Extract mel spectrogram\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "\n",
    "    num_freq_bins = mel_spectrogram.shape[0]\n",
    "    num_time_bins = mel_spectrogram.shape[1]\n",
    "\n",
    "    # 3. Apply masking\n",
    "    # Frequency masking\n",
    "    for _ in range(freq_mask_num):\n",
    "        freq_start = np.random.randint(0, num_freq_bins)\n",
    "        freq_length = np.random.randint(\n",
    "            0, int(num_freq_bins * freq_masking_max_percentage))\n",
    "        freq_end = min(mel_spectrogram.shape[0], freq_start + freq_length)\n",
    "        mel_spectrogram[freq_start:freq_end, :] = 0\n",
    "\n",
    "    # Time masking\n",
    "    for _ in range(time_mask_num):\n",
    "        time_start = np.random.randint(0, num_time_bins)\n",
    "        time_length = np.random.randint(\n",
    "            0, int(num_time_bins * time_masking_max_percentage))\n",
    "        time_end = min(mel_spectrogram.shape[1], time_start + time_length)\n",
    "        mel_spectrogram[:, time_start:time_end] = 0\n",
    "\n",
    "    return mel_spectrogram\n",
    "\n",
    "\n",
    "def show_masked_mel(db_masked_mel):\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.imshow(db_masked_mel, origin='lower', aspect='auto',\n",
    "               extent=[0, db_masked_mel.shape[1], 0, db_masked_mel.shape[0]])\n",
    "    plt.colorbar(format='%+2.0f dB')\n",
    "    plt.title('Realistically Masked Mel spectrogram (using matplotlib)')\n",
    "    plt.xlabel('Time frames')\n",
    "    plt.ylabel('Mel frequency bins')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def save_masked_spectrogram_to_wav(mel_spectrogram, sr, save_path):\n",
    "    \"\"\" Convert masked mel spectrogram back to waveform and save as wav \"\"\"\n",
    "    y_inv = librosa.feature.inverse.mel_to_audio(mel_spectrogram, sr=sr)\n",
    "    sf.write(save_path, y_inv, sr)\n",
    "\n",
    "\n",
    "def create_mask(wav_file, output_dir=None, n_create=2):\n",
    "    sr = 16000\n",
    "    wav_output_dir, wav_file_name = wav_file.rsplit('/', 1)\n",
    "    base_filename = wav_file_name.split('.')[0]\n",
    "\n",
    "    if output_dir == None:\n",
    "        output_dir = wav_output_dir\n",
    "\n",
    "    for i in range(1, n_create + 1, 1):\n",
    "        odd = i % 2 == 0\n",
    "        masked_mel = mask_melspectrogram(\n",
    "            wav_file,\n",
    "            freq_mask_num=1,\n",
    "            time_mask_num=1,\n",
    "            freq_masking_max_percentage=0.12 if odd else 0.07,\n",
    "            time_masking_max_percentage=0.07 if odd else 0.12\n",
    "        )\n",
    "        save_masked_spectrogram_to_wav(masked_mel, sr, os.path.join(\n",
    "            output_dir, base_filename + f\"_mask{i}.wav\"))\n",
    "        # show_masked_mel(librosa.power_to_db(masked_mel, ref=np.max))\n",
    "        # print(f'Save: {os.path.join(output_dir, base_filename + f\"_mask{i}.wav\")}')\n",
    "\n",
    "\n",
    "top_10 = int(len(os.listdir(os.path.join(data_path, state_list[0]))) * 0.1)\n",
    "\n",
    "for state in os.listdir(data_path):\n",
    "    if state == '.DS_Store':\n",
    "        continue\n",
    "\n",
    "    state_data_path = os.path.join(data_path, state)\n",
    "    file_list = os.listdir(state_data_path)\n",
    "    for i in tqdm(range(1, top_10 + 1, 1)):\n",
    "        create_mask(\n",
    "            wav_file=os.path.join(state_data_path, f'{state}_{i}.wav'),\n",
    "            output_dir=state_data_path,\n",
    "            n_create=1\n",
    "        )\n",
    "    print(f'State {state} masking done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
